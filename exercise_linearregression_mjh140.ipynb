{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Your Own Linear Regression Model\n",
    "\n",
    "One task that you will almost certainly be required to do other data science courses (especially if you are a MIDS student) is to write up some of your statistical / machine learning models from scratch. This can be a very valuable exercise, as it ensures that you understand what is actually going on behind the scenes of the models you use ever day, and that you don't just think of them as \"black boxes\". \n",
    "\n",
    "To get a little practice doing this, today you will be coding up your own linear regression model! \n",
    "\n",
    "(If you are using this site but aren't actually in this class, you are welcome to skip this exercise if you'd like -- this is more about practicing Python in anticipation of the requirements of other courses than developing your applied data science skills.) \n",
    "\n",
    "There are, broadly speaking, two approaches you can take to coding up your own model: \n",
    "\n",
    "1. you can write the model by defining a new function, or \n",
    "2. you can write the model by defining a new class with associated methods (making a model that works the way a model works in `scikit-learn`). \n",
    "\n",
    "Whether you do 1 or 2 is very much a matter of choice and style. Approach one, for example, is more consistent with what is called a *functional* style of programming, while approach two is more consistent with an *object-oriented* style of programming. Python can readily support both approaches, so either would work fine. \n",
    "\n",
    "In these exercises, however, I will ask you to use approach number 2 as this *tends* to be the more difficult approach, and so practicing approach 2 will be extra useful in preparing you for other classes (HA! Pun...). In particular, our goal is to implement a linear regression model that has the same \"initialize-fit-predict-score\" API (application programming interface -- a fancy name for the methods a class supports) as `scikit-learn` models. That means your model should be able to do all of the following:\n",
    "\n",
    "1. **Initialize** a new model. \n",
    "2. **Fit** a linear model when given a numpy vector (`y`) and a numpy matrix (`X`) with the syntax `my_model.fit(X, y)`. \n",
    "3. **Predict** values when given a new `numpy` matrix (`X_test`) with the syntax `my_model.predict(X_test)`. \n",
    "4. Return the **model coefficients** through the property `my_model.coefficients` (not quite what is used in `sklearn`, but let's use that interface). \n",
    "\n",
    "Also, bear in mind that throughout these exercises, we'll be working in `numpy` instead of `pandas`, just as we do in `scikit-learn`. So assume that before using your model, your user has already converted their data from `pandas` into `numpy` arrays. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** Define a new Class called `MyLinearModel` with methods for `__init__`, `fit`, `predict`, and an attribute for `coefficients`. For now, we don't need any initialization *arguments*, just an `__init__` function. \n",
    "\n",
    "As you get your code outline going, start by just having each method `pass`:\n",
    "\n",
    "```python\n",
    "def my_method(self):\n",
    "    pass\n",
    "```\n",
    "\n",
    "This will allow your methods to run without errors (they just don't do anything). Then we can double back to each method to get them working one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearModel:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # This method will be used to fit the model to the data\n",
    "        pass\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # This method will be used to make predictions on new data\n",
    "        pass\n",
    "\n",
    "    def coefficients(self):\n",
    "        # This attribute will hold the model coefficients\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** Now define your `fit` method. This is the method that should actually run your linear regression. In case you've forgotten your linear algebra, remember that for linear regressions, $\\beta = (X'X)^{-1}X'Y$, a fact you can see explained in detail on page four [here](https://www.stat.purdue.edu/~boli/stat512/lectures/topic3.pdf).\n",
    "\n",
    "Note that once you have written the code to do a linear regression, you'll need to put your outputs (your coefficents) somewhere. I recommend making an attribute for your class where you can store your coefficients. \n",
    "\n",
    "(As a reminder: the normal multiply operator (`*`) in `numpy` implies scalar multiplication. Use `@` for matrix multiplication). \n",
    "\n",
    "**HINT:** Remember that linear regressions require a vector of 1s in the `X` matrix. As the package writer, you get to decide whether users are expected to provide a matrix `X` that already has a vector of 1s, or whether you expect the user to provide a matrix `X` that doesn't have a vector of 1s (in which case you will need to add a vector of 1s before you fit the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class MyLinearModel:\n",
    "    def __init__(self):\n",
    "        # Placeholder for coefficients\n",
    "        self._coefficients = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Adding a column of ones to X for the intercept term\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        X_ones = np.hstack([intercept, X])\n",
    "\n",
    "        # Compute the coefficients using the normal equation\n",
    "        # beta = (X'X)^(-1)X'Y\n",
    "        self._coefficients = np.linalg.inv(X_ones.T @ X_ones) @ X_ones.T @ y\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # This method will be used to make predictions on new data\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def coefficients(self):\n",
    "        # This attribute will return the model coefficients\n",
    "        return self._coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)** As you write code, it is good to test your code as you work. With that in mind, let's create some toy data. First, create a 100 x 2 matrix where each column is normally distributed. Then create a vector `y` that is a linear combination of those two columns **plus** a vector of normally distributed noise **and** a constant term. \n",
    "\n",
    "In other words, we want to create data where we *know* exactly what coefficients we should see so when we test our regression, we know if the results are accurate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# 100 x 2 matrix\n",
    "X = np.random.normal(size=(100, 2))\n",
    "\n",
    "# True coefficients for the linear combination\n",
    "coefficients_true = np.array([3, 5])\n",
    "\n",
    "# Define true intercept\n",
    "intercept_true = 2\n",
    "\n",
    "# Normally distributed noise\n",
    "noise = np.random.normal(scale=1, size=100)\n",
    "\n",
    "y = intercept_true + X @ coefficients_true + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4)** Now test whether you `fit` method generates the correct coefficients. Remember the choice you made in Question 2 about whether your package expects the users' `X` matrix to include a vector of 1s when you test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated intercept and coefficients are: 1.9485888748112745, [3.1104649 4.9459629].\n",
      "The actual intercept and coefficients are: 2, [3 5].\n",
      "The model appears to be working correctly as the estimated coefficients are close to the actual coefficients. \n",
      "The only discrepancy is due to the noise included in the test data.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Defining the MyLinearModel class with the fit method implemented\n",
    "class MyLinearModel:\n",
    "    def __init__(self):\n",
    "        self._coefficients = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        X_with_intercept = np.hstack([intercept, X])\n",
    "        self._coefficients = (\n",
    "            np.linalg.inv(X_with_intercept.T @ X_with_intercept)\n",
    "            @ X_with_intercept.T\n",
    "            @ y\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def coefficients(self):\n",
    "        return self._coefficients\n",
    "\n",
    "\n",
    "# Generating the toy data\n",
    "np.random.seed(0)\n",
    "X = np.random.normal(size=(100, 2))\n",
    "coefficients_true = np.array([3, 5])\n",
    "intercept_true = 2\n",
    "noise = np.random.normal(scale=1, size=100)\n",
    "y = intercept_true + X @ coefficients_true + noise\n",
    "\n",
    "# Testing the fit method\n",
    "model = MyLinearModel()\n",
    "model.fit(X, y)\n",
    "estimated_coefficients = model.coefficients\n",
    "\n",
    "# Extracting and displaying the estimated coefficients\n",
    "intercept_estimated, coefficients_estimated = (\n",
    "    estimated_coefficients[0],\n",
    "    estimated_coefficients[1:],\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"The estimated intercept and coefficients are: {estimated_coefficients[0]}, {estimated_coefficients[1:]}.\"\n",
    ")\n",
    "print(\n",
    "    f\"The actual intercept and coefficients are: {intercept_true}, {coefficients_true}.\"\n",
    ")\n",
    "print(\n",
    "    \"\"\"The model appears to be working correctly as the estimated coefficients are close to the actual coefficients. \n",
    "The only discrepancy is due to the noise included in the test data.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5)** Now let's make the statisticians proud, and in addition to storing the coefficients, let's store the standard errors for our estimated coefficients as another attribute. Recall that the simplest method of calculating the variance covariance matrix for $\\beta$ is using the formula $\\sigma^2 (X'X)^{-1}$, where $\\sigma^2$ is the variance of the error terms of your regression. The standard errors for your coefficient estimates will be the diagonal values of that matrix. See page six [here](https://www.stat.purdue.edu/~boli/stat512/lectures/topic3.pdf) for a full derivation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Errors: [0.09674321 0.09376987 0.09433688]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class MyLinearModel:\n",
    "    def __init__(self):\n",
    "        self._coefficients = None\n",
    "        self._standard_errors = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        X_with_intercept = np.hstack([intercept, X])\n",
    "\n",
    "        # Calculate coefficients\n",
    "        self._coefficients = (\n",
    "            np.linalg.inv(X_with_intercept.T @ X_with_intercept)\n",
    "            @ X_with_intercept.T\n",
    "            @ y\n",
    "        )\n",
    "\n",
    "        # Calculating the standard errors\n",
    "        residuals = y - X_with_intercept @ self._coefficients\n",
    "        error_variance = np.var(residuals, ddof=X_with_intercept.shape[1])\n",
    "        covariance_matrix = error_variance * np.linalg.inv(\n",
    "            X_with_intercept.T @ X_with_intercept\n",
    "        )\n",
    "        self._standard_errors = np.sqrt(np.diag(covariance_matrix))\n",
    "\n",
    "    @property\n",
    "    def coefficients(self):\n",
    "        return self._coefficients\n",
    "\n",
    "    @property\n",
    "    def standard_errors(self):\n",
    "        return self._standard_errors\n",
    "\n",
    "\n",
    "# Generating the toy data\n",
    "np.random.seed(0)\n",
    "X = np.random.normal(size=(100, 2))\n",
    "coefficients_true = np.array([3, 5])\n",
    "intercept_true = 2\n",
    "noise = np.random.normal(scale=1, size=100)\n",
    "y = intercept_true + X @ coefficients_true + noise\n",
    "\n",
    "# Testing the fit method\n",
    "model = MyLinearModel()\n",
    "model.fit(X, y)\n",
    "\n",
    "# # Access the standard errors\n",
    "standard_errors = model.standard_errors\n",
    "print(\"Standard Errors:\", standard_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(6)** Now let's also add an R-squared attribute to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The R-Squared Value is 0.9185177413723371.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class MyLinearModel:\n",
    "    def __init__(self):\n",
    "        self._coefficients = None\n",
    "        self._standard_errors = None\n",
    "        self._r_squared = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        X_with_intercept = np.hstack([intercept, X])\n",
    "\n",
    "        # Calculate coefficients\n",
    "        self._coefficients = (\n",
    "            np.linalg.inv(X_with_intercept.T @ X_with_intercept)\n",
    "            @ X_with_intercept.T\n",
    "            @ y\n",
    "        )\n",
    "\n",
    "        # Calculating the standard errors\n",
    "        residuals = y - X_with_intercept @ self._coefficients\n",
    "        error_variance = np.var(residuals, ddof=X_with_intercept.shape[1])\n",
    "        covariance_matrix = error_variance * np.linalg.inv(\n",
    "            X_with_intercept.T @ X_with_intercept\n",
    "        )\n",
    "        self._standard_errors = np.sqrt(np.diag(covariance_matrix))\n",
    "\n",
    "        # Calculating R-squared\n",
    "        ss_total = np.sum((y - np.mean(y)) ** 2)\n",
    "        ss_res = np.sum(residuals**2)\n",
    "        self._r_squared = 1 - (ss_res / ss_total)\n",
    "\n",
    "    @property\n",
    "    def coefficients(self):\n",
    "        return self._coefficients\n",
    "\n",
    "    @property\n",
    "    def standard_errors(self):\n",
    "        return self._standard_errors\n",
    "\n",
    "    @property\n",
    "    def r_squared(self):\n",
    "        return self._r_squared\n",
    "\n",
    "\n",
    "# Testing the updated MyLinearModel class with R-squared calculation\n",
    "model = MyLinearModel()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Displaying the R-squared value\n",
    "r_squared_value = model.r_squared\n",
    "print(f\"The R-Squared Value is {r_squared_value}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(7)** Now we'll go ahead and cheat a little. Use `statsmodels` to fit your model with your toy data to ensure your standard errors and r-squared are correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Linear Model Standard Errors: [0.09674321 0.09376987 0.09433688]\n",
      "Manual Linear Model R-squared: 0.9185177413723371\n",
      "Statsmodels Standard Errors: [0.09674321 0.09376987 0.09433688]\n",
      "Statsmodels R-squared: 0.9185177413723371\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "class MyLinearModel:\n",
    "    def __init__(self):\n",
    "        self._coefficients = None\n",
    "        self._standard_errors = None\n",
    "        self._r_squared = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        X_with_intercept = np.hstack([intercept, X])\n",
    "\n",
    "        # Calculate coefficients\n",
    "        self._coefficients = (\n",
    "            np.linalg.inv(X_with_intercept.T @ X_with_intercept)\n",
    "            @ X_with_intercept.T\n",
    "            @ y\n",
    "        )\n",
    "\n",
    "        # Calculating the standard errors\n",
    "        residuals = y - X_with_intercept @ self._coefficients\n",
    "        error_variance = np.var(residuals, ddof=X_with_intercept.shape[1])\n",
    "        covariance_matrix = error_variance * np.linalg.inv(\n",
    "            X_with_intercept.T @ X_with_intercept\n",
    "        )\n",
    "        self._standard_errors = np.sqrt(np.diag(covariance_matrix))\n",
    "\n",
    "        # Calculating R-squared\n",
    "        ss_total = np.sum((y - np.mean(y)) ** 2)\n",
    "        ss_res = np.sum(residuals**2)\n",
    "        self._r_squared = 1 - (ss_res / ss_total)\n",
    "\n",
    "    @property\n",
    "    def coefficients(self):\n",
    "        return self._coefficients\n",
    "\n",
    "    @property\n",
    "    def standard_errors(self):\n",
    "        return self._standard_errors\n",
    "\n",
    "    @property\n",
    "    def r_squared(self):\n",
    "        return self._r_squared\n",
    "\n",
    "\n",
    "# Testing the updated MyLinearModel class with R-squared calculation\n",
    "model = MyLinearModel()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Use statsmodels to fit the model for validation\n",
    "X_with_ones = sm.add_constant(X)\n",
    "statsmodel = sm.OLS(y, X_with_ones).fit()\n",
    "\n",
    "\n",
    "# Compare standard errors and R-squared with statsmodels\n",
    "print(\"Manual Linear Model Standard Errors:\", model.standard_errors)\n",
    "print(\"Manual Linear Model R-squared:\", model.r_squared)\n",
    "print(\"Statsmodels Standard Errors:\", statsmodel.bse)\n",
    "print(\"Statsmodels R-squared:\", statsmodel.rsquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(8)** Now implement `predict`! Then test it against your original `X` data -- do you get back something very close to your true `y`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True y:\n",
      " [ 7.761232    9.24107354  7.00945439  5.7580418   4.1211617   4.3608452\n",
      "  5.09728322  3.97224096  6.64583741  2.14714653 -1.29578216  4.14241168\n",
      "  6.84285631  2.31104627  8.53234757  3.65801141 -2.80191843  2.79079951\n",
      " 10.49931938  1.44528085 -3.1333597   1.89973846  0.47373983  1.74897571\n",
      " -2.95847414  1.52102009 -0.24564424  3.93272283  3.1134264   0.52333894\n",
      " -0.75240772 -2.63349427  2.17749819  0.17356224 -0.33092663  5.83609375\n",
      "  5.12029978  0.70321091  0.20733646  2.99912432  1.07117605  1.00164565\n",
      " 10.00035042  5.32529334  1.91211859  2.51645495  6.43096928  6.24236629\n",
      "  5.42745268  2.57443676  5.80297531  0.99291361  3.70766278  0.30120547\n",
      " 11.16382044  9.99452693  3.55945532  2.39104913  3.50617335  6.80554981\n",
      "  0.77328315  5.6262737   0.39792414  5.18822354  3.47788839  2.23463696\n",
      "  1.89535438  2.71638682  3.75565311  2.11990623  0.46180743  1.78714757\n",
      " 12.61383732  3.11866708 -1.7730294   5.52245951  0.14426535  1.44350383\n",
      "  1.84202862  1.40774515  5.11753591  6.55131696  0.76421471 -0.74474929\n",
      "  6.82691487  6.48470067  2.9249771   2.85240832  0.4401044   3.19447746\n",
      " -0.98798467 -3.0921988   3.98329155 -0.72707703  3.79256908  1.38544065\n",
      "  0.89694963  4.58236193  6.01233483 11.18389607]\n",
      "\n",
      "Predicted y: \n",
      " [ 8.25224525  9.41606642  6.11749254  5.16449727  3.39602944  5.86371715\n",
      "  5.14769121  4.68986126  6.49063114  2.27462165 -2.57470762  3.90937664\n",
      "  6.33443515  2.74122382  9.51530112  4.01333142 -2.82734181  2.31335592\n",
      "  8.25953012  1.40967643 -2.26792047  1.9004119   0.85329826  1.03567987\n",
      " -1.63587145  1.28652225 -0.44619     3.62287171  3.65033446  0.65938069\n",
      "  0.565388   -2.18812633  2.71025898 -0.49898604  0.67081307  5.07705357\n",
      "  3.7669943   2.8032346  -0.33524656  2.23335453  1.47981574  1.48195985\n",
      " 10.14377116  5.71160948  1.99446033  4.03045335  5.19745892  5.11252138\n",
      "  6.09406244  3.98174695  5.51135036  1.32456307  3.27968188  0.56382267\n",
      " 10.55328544  9.40570635  4.035292    3.65003458  5.15071502  6.16176015\n",
      "  2.01172152  6.04294973  0.8741875   5.04138166  5.41500399  1.88025907\n",
      "  1.24384211  2.46976244  4.09007707  2.07390566 -0.17712512  4.49242551\n",
      " 10.81873081  2.51596749 -1.29245083  5.76187939 -0.43849848  1.53317354\n",
      "  4.00411206 -0.81125532  5.01738402  5.57987979  1.22385889 -2.35955121\n",
      "  6.60326899  5.90894517  4.04864858  1.43877298 -0.35261219  1.78995523\n",
      " -0.37611375 -2.61353379  1.7836004   0.15671322  3.71203187  0.0398922\n",
      "  0.80659455  4.0150707   6.40112872 10.92916861]\n",
      "\n",
      "Differences (True y - Predicted y): \n",
      " [-4.91013248e-01 -1.74992879e-01  8.91961848e-01  5.93544527e-01\n",
      "  7.25132264e-01 -1.50287196e+00 -5.04079921e-02 -7.17620300e-01\n",
      "  1.55206268e-01 -1.27475114e-01  1.27892546e+00  2.33035035e-01\n",
      "  5.08421162e-01 -4.30177554e-01 -9.82953550e-01 -3.55320014e-01\n",
      "  2.54233806e-02  4.77443587e-01  2.23978926e+00  3.56044286e-02\n",
      " -8.65439226e-01 -6.73435932e-04 -3.79558436e-01  7.13295840e-01\n",
      " -1.32260269e+00  2.34497834e-01  2.00545764e-01  3.09851121e-01\n",
      " -5.36908058e-01 -1.36041757e-01 -1.31779572e+00 -4.45367939e-01\n",
      " -5.32760789e-01  6.72548283e-01 -1.00173970e+00  7.59040172e-01\n",
      "  1.35330548e+00 -2.10002368e+00  5.42583021e-01  7.65769792e-01\n",
      " -4.08639686e-01 -4.80314198e-01 -1.43420733e-01 -3.86316141e-01\n",
      " -8.23417470e-02 -1.51399839e+00  1.23351036e+00  1.12984490e+00\n",
      " -6.66609756e-01 -1.40731020e+00  2.91624953e-01 -3.31649457e-01\n",
      "  4.27980903e-01 -2.62617202e-01  6.10534999e-01  5.88820577e-01\n",
      " -4.75836672e-01 -1.25898545e+00 -1.64454166e+00  6.43789661e-01\n",
      " -1.23843837e+00 -4.16676033e-01 -4.76263359e-01  1.46841872e-01\n",
      " -1.93711560e+00  3.54377888e-01  6.51512271e-01  2.46624381e-01\n",
      " -3.34423955e-01  4.60005748e-02  6.38932548e-01 -2.70527795e+00\n",
      "  1.79510651e+00  6.02699586e-01 -4.80578567e-01 -2.39419876e-01\n",
      "  5.82763829e-01 -8.96697078e-02 -2.16208344e+00  2.21900047e+00\n",
      "  1.00151890e-01  9.71437175e-01 -4.59644181e-01  1.61480192e+00\n",
      "  2.23645879e-01  5.75755498e-01 -1.12367149e+00  1.41363534e+00\n",
      "  7.92716590e-01  1.40452223e+00 -6.11870912e-01 -4.78665007e-01\n",
      "  2.19969114e+00 -8.83790250e-01  8.05372124e-02  1.34554845e+00\n",
      "  9.03550806e-02  5.67291234e-01 -3.88793896e-01  2.54727460e-01]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class MyLinearModel:\n",
    "    def __init__(self):\n",
    "        self._coefficients = None\n",
    "        self._standard_errors = None\n",
    "        self._r_squared = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        X_with_intercept = np.hstack([intercept, X])\n",
    "\n",
    "        # Calculate coefficients\n",
    "        self._coefficients = (\n",
    "            np.linalg.inv(X_with_intercept.T @ X_with_intercept)\n",
    "            @ X_with_intercept.T\n",
    "            @ y\n",
    "        )\n",
    "\n",
    "        # Calculating the standard errors\n",
    "        residuals = y - X_with_intercept @ self._coefficients\n",
    "        error_variance = np.var(residuals, ddof=X_with_intercept.shape[1])\n",
    "        covariance_matrix = error_variance * np.linalg.inv(\n",
    "            X_with_intercept.T @ X_with_intercept\n",
    "        )\n",
    "        self._standard_errors = np.sqrt(np.diag(covariance_matrix))\n",
    "\n",
    "        # Calculating R-squared\n",
    "        ss_total = np.sum((y - np.mean(y)) ** 2)\n",
    "        ss_res = np.sum(residuals**2)\n",
    "        self._r_squared = 1 - (ss_res / ss_total)\n",
    "\n",
    "    @property\n",
    "    def coefficients(self):\n",
    "        return self._coefficients\n",
    "\n",
    "    @property\n",
    "    def standard_errors(self):\n",
    "        return self._standard_errors\n",
    "\n",
    "    @property\n",
    "    def r_squared(self):\n",
    "        return self._r_squared\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Add a column of 1s to the beginning of X\n",
    "        X_with_ones = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "\n",
    "        # Predict y using the fitted coefficients\n",
    "        y_pred = X_with_ones @ self.coefficients\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# Generating the toy data\n",
    "np.random.seed(0)\n",
    "X = np.random.normal(size=(100, 2))\n",
    "coefficients_true = np.array([3, 5])\n",
    "intercept_true = 2\n",
    "noise = np.random.normal(scale=1, size=100)\n",
    "y = intercept_true + X @ coefficients_true + noise\n",
    "\n",
    "# Testing the updated MyLinearModel class with R-squared calculation\n",
    "model = MyLinearModel()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict y using the original X data\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Calculate the differences between true y and predicted y\n",
    "differences = y - y_pred\n",
    "\n",
    "# Print true y, predicted y, and differences\n",
    "print(\n",
    "    f\"True y:\\n {y}\",\n",
    ")\n",
    "print(f\"\\nPredicted y: \\n {y_pred}\")\n",
    "print(f\"\\nDifferences (True y - Predicted y): \\n {differences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(9)** Finally, create the *option* of fitting the model with or without a constant term. In other words, create an option so that, if the user passes a numpy array *without* a constant term, your code will add a vector of 1s before fitting the model. As in `scikit-learn`, make this an option you set during initialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\matth\\OneDrive\\Documents\\Duke MIDS\\Courses\\IDS720 - Practical Data Science\\linear_regression\\exercise_linearregression.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/matth/OneDrive/Documents/Duke%20MIDS/Courses/IDS720%20-%20Practical%20Data%20Science/linear_regression/exercise_linearregression.ipynb#X24sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m model_nointercept \u001b[39m=\u001b[39m MyLinearModel(fit_intercept\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/matth/OneDrive/Documents/Duke%20MIDS/Courses/IDS720%20-%20Practical%20Data%20Science/linear_regression/exercise_linearregression.ipynb#X24sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m \u001b[39m# Predict y using the original X data for both models\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/matth/OneDrive/Documents/Duke%20MIDS/Courses/IDS720%20-%20Practical%20Data%20Science/linear_regression/exercise_linearregression.ipynb#X24sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m y_pred_intercept \u001b[39m=\u001b[39m model_intercept\u001b[39m.\u001b[39;49mpredict(X)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/matth/OneDrive/Documents/Duke%20MIDS/Courses/IDS720%20-%20Practical%20Data%20Science/linear_regression/exercise_linearregression.ipynb#X24sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m y_pred_nointercept \u001b[39m=\u001b[39m model_nointercept\u001b[39m.\u001b[39mpredict(X)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/matth/OneDrive/Documents/Duke%20MIDS/Courses/IDS720%20-%20Practical%20Data%20Science/linear_regression/exercise_linearregression.ipynb#X24sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m \u001b[39m# Compare predicted y with true y for both models\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\matth\\OneDrive\\Documents\\Duke MIDS\\Courses\\IDS720 - Practical Data Science\\linear_regression\\exercise_linearregression.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/matth/OneDrive/Documents/Duke%20MIDS/Courses/IDS720%20-%20Practical%20Data%20Science/linear_regression/exercise_linearregression.ipynb#X24sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m     X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mhstack((intercept_column, X))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/matth/OneDrive/Documents/Duke%20MIDS/Courses/IDS720%20-%20Practical%20Data%20Science/linear_regression/exercise_linearregression.ipynb#X24sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m \u001b[39m# Predict the target values using the fitted coefficients\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/matth/OneDrive/Documents/Duke%20MIDS/Courses/IDS720%20-%20Practical%20Data%20Science/linear_regression/exercise_linearregression.ipynb#X24sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m y_pred \u001b[39m=\u001b[39m X \u001b[39m@\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_coefficients\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/matth/OneDrive/Documents/Duke%20MIDS/Courses/IDS720%20-%20Practical%20Data%20Science/linear_regression/exercise_linearregression.ipynb#X24sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m \u001b[39mreturn\u001b[39;00m y_pred\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class MyLinearModel:\n",
    "    def __init__(self, fit_intercept=True):\n",
    "        self._coefficients = None\n",
    "        self._standard_errors = None\n",
    "        self._r_squared = None\n",
    "        self.fit_intercept = fit_intercept\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            # Add a column of ones to X for the intercept term if fit_intercept is True\n",
    "            intercept = np.ones((X.shape[0], 1))\n",
    "            X = np.hstack([intercept, X])\n",
    "\n",
    "        # Calculate coefficients\n",
    "        self._coefficients = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "        # Calculating the standard errors\n",
    "        residuals = y - X @ self._coefficients\n",
    "        error_variance = np.var(residuals, ddof=X.shape[1])\n",
    "        covariance_matrix = error_variance * np.linalg.inv(X.T @ X)\n",
    "        self._standard_errors = np.sqrt(np.diag(covariance_matrix))\n",
    "\n",
    "        # Calculating R-squared\n",
    "        ss_total = np.sum((y - np.mean(y)) ** 2)\n",
    "        ss_res = np.sum(residuals**2)\n",
    "        self._r_squared = 1 - (ss_res / ss_total)\n",
    "\n",
    "    @property\n",
    "    def coefficients(self):\n",
    "        return self._coefficients\n",
    "\n",
    "    @property\n",
    "    def standard_errors(self):\n",
    "        return self._standard_errors\n",
    "\n",
    "    @property\n",
    "    def r_squared(self):\n",
    "        return self._r_squared\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.fit_intercept:\n",
    "            intercept_column = np.ones((X.shape[0], 1))\n",
    "            X = np.hstack((intercept_column, X))\n",
    "\n",
    "        # Predict the target values using the fitted coefficients\n",
    "        y_pred = X @ self._coefficients\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# Generating the toy data\n",
    "np.random.seed(0)\n",
    "X = np.random.normal(size=(100, 2))\n",
    "coefficients_true = np.array([3, 5])\n",
    "intercept_true = 2\n",
    "noise = np.random.normal(scale=1, size=100)\n",
    "y = intercept_true + X @ coefficients_true + noise\n",
    "\n",
    "# Test the MyLinearModel with and without intercept\n",
    "model_intercept = MyLinearModel(fit_intercept=True)\n",
    "model_nointercept = MyLinearModel(fit_intercept=False)\n",
    "\n",
    "# Predict y using the original X data for both models\n",
    "y_pred_intercept = model_intercept.predict(X)\n",
    "y_pred_nointercept = model_nointercept.predict(X)\n",
    "\n",
    "# Compare predicted y with true y for both models\n",
    "print(f\"True y:\\n {y}\")\n",
    "print(f\"\\nPredicted y with intercept:\\n {y_pred_intercept}\")\n",
    "print(f\"\\nPredicted y without intercept:\\n {y_pred_nointercept}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
